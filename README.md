# Post Training Quantization Model Compression

This project demonstrates how to compress a trained Keras model using post-training quantization with TensorFlow Lite, reducing model size and enabling faster, resource-efficient inferenceâ€”ideal for deployment on mobile and embedded systems.

# What Can a Quantized Model (Compression) Do for Me?
  * Reduce model size significantly without retraining
  * Speed up inference on edge devices
  * Lower memory and power usage

# Getting Started
### 1. Create an Environment
``` conda create -n compressed_model python==3.12 ```
